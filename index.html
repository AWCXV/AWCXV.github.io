<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" type="image/x-icon" href="https://github.com/AWCXV/AWCXV.github.io/blob/main/images/ccy_ico.ico">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="/img/img_ico.ico" />
<link rel="bookmark" href="/img/img_ico.ico" type="image/x-icon"　/>
<title>Chunyang Cheng 程春阳</title>
</head>
<body>
<td id="layout-content">
<div id="toptitle">
<h1>Chunyang Cheng 程春阳</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://awcxv.github.io"><img src="/images/ccy.jpg" alt="alt text" height="170px" /></a>&nbsp;</td>  
<td align="left">
Status: Third year PhD student <br />
<b>Advisors: Xiao-Jun Wu & Josef Kittler </b> <br />
Research Interest: Image Fusion <br />
Institution: Jiangnan University <br />
Location: Wuxi, China <br />
E-mail: chunyang_cheng@163.com<br />
<div class="menu-item"><a href="https://scholar.google.com.hk/citations?user=tkTfP6wAAAAJ&hl=zh-CN">Google Scholar</a></div>
<div class="menu-item"><a href="https://github.com/AWCXV">Code Repository</a></div>
</td></tr></table>

<h3><font color=blue>Experience </font></h3>

<li>2016 ~ 2020  &ensp; <b>Bachelor Degree</b> &ensp;School of Computer and Information Engineering &ensp; <b>Henan University</b>, Kaifeng, China  </li>
<li>2020 ~ 2022  &ensp; <b>Master Student </b> &ensp;International Joint Laboratory on Artificial Intelligence of Jiangsu Province &ensp; <b>Jiangnan University</b>, Wuxi, China  </li>
<li>2022 ~ 2023  &ensp; <b>PhD Student </b> &ensp;International  Joint Laboratory on Artificial Intelligence of Jiangsu Province &ensp; <b>Jiangnan University</b>, Wuxi, China </li>
<li>2023 ~ 2024  &ensp; <b>Visiting Research PhD Student </b> &ensp; Centre for Vision, Speech and Signal Processing (CVSSP) &ensp; <b>University of Surrey</b>, Guildford, UK </li>
<li>2024 ~ now  &ensp; <b>PhD Student </b> &ensp; International Joint Laboratory on Artificial Intelligence of Jiangsu Province &ensp; <b>Jiangnan University</b>, Wuxi, China </li>

<h3><font color=blue> Publications </font></h3>
<ul>
</li>

<li><p>"SMLNet: A SPD Manifold Learning Network for Infrared and Visible Image Fusion."  <b>IJCV</b>, 2025. [<a href="https://arxiv.org/abs/2411.10679"><font color=blue>paper</font></a>] [<a href="https://github.com/Shaoyun2023/SMLNet"><font color=blue>Code</font></a>]
</li>
  
<li><p>"Revisiting RGBT Tracking Benchmarks from the Perspective of Modality Validity: A New Benchmark, Problem, and Solution."  <b>IEEE TIP</b>, 2025. [<a href="https://arxiv.org/abs/2405.00168"><font color=blue>paper</font></a>] [<a href="https://github.com/Zhangyong-Tang/MVRGBT"><font color=blue>Code</font></a>]
</li>
  
<li><p>"S4Fusion: Saliency-aware Selective State Space Model for Infrared Visible Image Fusion."  <b>IEEE TIP</b>, 2025. [<a href="https://ieeexplore.ieee.org/abstract/document/11062462"><font color=blue>paper</font></a>] [<a href="https://github.com/zipper112/S4Fusion"><font color=blue>Code</font></a>]
</li>
  
<li><p>"Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking."  <b>ACM MM</b>, 2025. [<a href="https://awcxv.github.io/"><font color=blue>paper(not available)</font></a>]
</li>
  
<li><p>"GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion."  <b>Information Fusion</b>, 2025. [<a href="https://www.sciencedirect.com/science/article/pii/S1566253525004750?via%3Dihub"><font color=blue>paper</font></a>]
</li>
  
<li><p>"One Model for ALL: Low-Level Task Interaction Is a Key to Task-Agnostic Image Fusion." <b>CVPR</b>, 2025. [<a href="https://arxiv.org/abs/2502.19854"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/GIFNet"><font color=blue>Code</font></a>]</p> 
</li>

<li><p>"TextFusion: Unveiling the power of textual semantics for controllable image fusion." <b>Information Fusion</b>, 2025. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253524005682"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/TextFusion"><font color=blue>Code</font></a>]</p> 
</li>

<li><p>"FusionBooster: A Unified Image Fusion Boosting Paradigm." <b>IJCV</b>, 2025. [<a href="https://arxiv.org/abs/2305.05970"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/FusionBooster"><font color=blue>Code</font></a>]</p> 
</li>

<li><p>"Conti-Fuse: A novel continuous decomposition-based fusion framework for infrared and visible images." <b>Information Fusion</b>, 2025. [<a href="https://www.sciencedirect.com/science/article/pii/S1566253524006171"><font color=blue>paper</font></a>] [<a href="https://github.com/zipper112/Conti-Fuse"><font color=blue>Code</font></a>]</p> 
</li>  

<li><p>"SMFuse: Two-Stage Structural Map Aware Network for Multi-focus Image Fusion." <b>ICPR</b>, 2025.  [<a href="https://link.springer.com/chapter/10.1007/978-3-031-78312-8_1"><font color=blue>paper</font></a>] </p></li>

<li><p>"MMDRFuse: Distilled Mini-Model with Dynamic Refresh for Multi-Modality Image Fusion." <b>ACM MM (oral)</b>, 2024. [<a href="https://arxiv.org/abs/2408.15641"><font color=blue>paper</font></a>] [<a href="https://github.com/yanglinDeng/MMDRFuse"><font color=blue>Code</font></a>]</p> 
</li>

<li><p>"Learning Feature Restoration Transformer for Robust Dehazing Visual Object Tracking." <b>IJCV</b>, 2024. [<a href="https://link.springer.com/article/10.1007/s11263-024-02182-9"><font color=blue>paper</font></a>]</p> 
</li>

<li><p>"DePF: A novel fusion approach based on decomposition pooling for infrared and visible images." <b>IEEE TIM</b>, 2023. [<a href="https://ieeexplore.ieee.org/document/10288477"><font color=blue>paper</font></a>] [<a href="https://github.com/draymondbiao/DePF"><font color=blue>code</font></a>] </p></li>
  
<li><p>"LE2Fusion: A novel local edge enhancement module for infrared and visible image fusion." <b>ICIG</b>, 2023. [<a href="https://link.springer.com/chapter/10.1007/978-3-031-46305-1_24"><font color=blue>paper</font></a>] [<a href="https://github.com/hli1221/LE2Fusion"><font color=blue>code</font></a>]</p></li>
  
<li><p>"MUFusion: A general unsupervised image fusion network based on memory unit". <b>Information Fusion</b>, 2023. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253522002202"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/MUFusion"><font color=blue>Code</font></a>]</p>
</li>  

<li><p>"UNIFusion: A lightweight unified image fusion network". <b>IEEE TIM</b>, 2021. [<a href="https://ieeexplore.ieee.org/document/9550760"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/UNIFusion"><font color=blue>Code</font></a>]</p>
</li>

</ul>
<h3><font color=blue> Awards & Projects </font></h3>
<ul>

<li><p>2025 President's Special Award, Jiangnan University</p>
</li>  
<li><p>2025 Top Ten Graduate Students, Jiangnan University</p>
</li>
<li><p><b>Chair, Completed</b>, 2023 Postgraduate Research & Practice Innovation Program of Jiangsu Province</p>
</li>
<li><p>2021 National Scholarship for Graduate Students</p>
</li>
<li><p><b>Silver Medal</b>, 2019 ACM/ICPC International College Programming Competition Regional</p>
</li>
<li><p><b>Silver Medal</b>, 2018 China College Student Programming Competition (CCPC)</p>
</li>

</ul>
</td>
</tr>
</table>
</body>
</html>
  
