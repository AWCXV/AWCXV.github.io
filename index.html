<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" type="image/x-icon" href="https://github.com/XU-TIANYANG/XU-TIANYANG.github.io/blob/main/img/img_ico.ico?">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="/img/img_ico.ico" />
<link rel="bookmark" href="/img/img_ico.ico" type="image/x-icon"　/>
<title>Chunyang Cheng 程春阳</title>
</head>
<body>
<td id="layout-content">
<div id="toptitle">
<h1>Chunyang Cheng 程春阳</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://awcxv.github.io"><img src="/images/ccy.jpg" alt="alt text" height="180px" /></a>&nbsp;</td>  
<td align="left"><p></b>Chunyang Cheng</b> <br />
3-rd year PhD student<br />
School of Artificial Intelligence and Computer Science<br />
Jiangnan University <br />
Wuxi, China <br />
E-mail: chunyang_cheng@163.com<br />
<div class="menu-item"><a href="https://scholar.google.com.hk/citations?user=tkTfP6wAAAAJ&hl=zh-CN">Google Scholar</a></div>
</td></tr></table>
<p>I am a third-year PhD student from Jiangnan University. I was a visiting research PhD at the Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guildford, United Kingdom, from 2023 to 2024.  My current reserach topic is Image Fusion.  </p>
<h3><font color=blue> Publications </font></h3>
<ul>
</li>
<li><p>One Model for ALL: Low-Level Task Interaction Is a Key to Task-Agnostic Image Fusion. <b>CVPR</b>, 2025. <b></b>Chunyang Cheng</b>, Tianyang Xu, Zhenhua Feng, Xiaojun Wu, ZhangyongTang, Hui Li, Zeyang Zhang, Sara Atito, Muhammad Awais, Josef Kittler. [<a href="https://arxiv.org/abs/2502.19854"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/GIFNet"><font color=blue>Code</font></a>]</p>  </p> 
</li>

<li><p>FusionBooster: A Unified Image Fusion Boosting Paradigm. <b>IJCV</b>, 2025. [<a href="https://arxiv.org/abs/2305.05970"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/FusionBooster"><font color=blue>Code</font></a>]</p> 
</li>

<li><p>TextFusion: Unveiling the power of textual semantics for controllable image fusion. <b>Information Fusion</b>, 2025. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253524005682"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/TextFusion"><font color=blue>Code</font></a>]</p> 
</li>


<li><p>MMDRFuse: Distilled Mini-Model with Dynamic Refresh for Multi-Modality Image Fusion. <b>ACM MM</b>, 2024. (<font color=blue><b>oral</b></font>) [<a href="https://arxiv.org/abs/2408.15641"><font color=blue>paper</font></a>] [<a href="https://github.com/yanglinDeng/MMDRFuse"><font color=blue>Code</font></a>]</p> 
</li>

<li><p>MUFusion: A general unsupervised image fusion network based on memory unit. <b>Information Fusion</b>, 2023. [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253522002202"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/MUFusion"><font color=blue>Code</font></a>]</p>
</li>

<li><p>Unifusion: A lightweight unified image fusion network. <b>IEEE TIM</b>, 2021. [<a href="https://ieeexplore.ieee.org/document/9550760"><font color=blue>paper</font></a>] [<a href="https://github.com/AWCXV/UNIFusion"><font color=blue>Code</font></a>]</p>
</li>

</ul>
<h3><font color=blue> Awards </font></h3>
<ul>

<li><p><b>2021年硕士研究生国家奖学金</b>, Make-up Dense Video Captioning Challenge (ACM MM2022)</p>
</li>
<li><p><b>Silver Award</b>, 2019 ACM/ICPC International College Programming Competition Regional</p>
</li>
<li><p><b>Silver Award</b>, 2019 ACM/ICPC International College Programming Competition Regional</p>
</li>
<li><p><b>Silver Award</b>, 2018 China College Student Programming Competition (CCPC)</p>
</li>

</ul>
</td>
</tr>
</table>
</body>
</html>
  
